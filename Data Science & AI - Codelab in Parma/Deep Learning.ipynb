{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This tutorial shows how a H2O Deep Learning model can be used to do supervised classification and regression. This tutorial covers usage of H2O from Python. An R version of this tutorial will be available as well in a separate document. This file is available in plain R, R markdown, regular markdown, plain Python and iPython Notebook formats, and the plots are available as PDF files. All documents are available on Github. More examples and explanations can be found in our H2O Deep Learning booklet and on our H2O Github Repository.\n",
    "\n",
    "## H2O Python Module\n",
    "Load the H2O Python module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start H2O\n",
    "Start up a 1-node H2O cloud on your local machine, and allow it to use all CPU cores and up to 2GB of memory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()            #uses all cores by default\n",
    "h2o.remove_all()      #clean slate, in case cluster was already running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.deeplearning import H2OAutoEncoderEstimator, H2ODeepLearningEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O Deep Learning\n",
    "While H2O Deep Learning has many parameters, it was designed to be just as easy to use as the other supervised training methods in H2O. Early stopping, automatic data standardization and handling of categorical variables and missing values and adaptive learning rates (per weight) reduce the amount of parameters the user has to specify. Often, it's just the number and sizes of hidden layers, the number of epochs and the activation function and maybe some regularization techniques.\n",
    "\n",
    "### Let's have some fun first: Decision Boundaries\n",
    "We start with a small dataset representing red and black dots on a plane, arranged in the shape of two nested spirals. Then we task H2O's machine learning methods to separate the red and black dots, i.e., recognize each spiral as such by assigning each point in the plane to one of the two spirals.\n",
    "\n",
    "We visualize the nature of H2O Deep Learning (DL), H2O's tree methods (GBM/DRF) and H2O's generalized linear modeling (GLM) by plotting the decision boundary between the red and black spirals:\n",
    "\n",
    "Let's get our imports first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline                         \n",
    "#IMPORT ALL THE THINGS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from h2o.estimators.deeplearning import H2OAutoEncoderEstimator, H2ODeepLearningEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to upload our datasets to the the H2O cluster. The data is imported into H2OFrames, which operate similarly in function to pandas DataFrames.\n",
    "\n",
    "In this case, the cluster is running on our laptops. Data files are imported by their relative locations to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiral = h2o.import_file(path = os.path.realpath(\"data/spiral.csv\"))\n",
    "grid  = h2o.import_file(path = os.path.realpath(\"data/grid.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiral is a simple data set consisting of two spirals of black and red dots.\n",
    "Grid is a 201 by 201 matrix with dimensions [-1.5, 1.5] by [-1.5, 1.5].\n",
    "\n",
    "To visualize these datasets, we can pull them from H2OFrames into pandas DataFrames for easier plotting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiral_df = spiral.as_data_frame(use_pandas=True)\n",
    "grid_df = grid.as_data_frame(use_pandas=True)\n",
    "grid_x, grid_y = grid_df.x.values.reshape(201,201), grid_df.y.values.reshape(201,201)\n",
    "spiral_r = spiral_df[spiral_df.color == \"Red\"]\n",
    "spiral_k = spiral_df[spiral_df.color == \"Black\"]\n",
    "\n",
    "spiral_xr, spiral_yr = spiral_r[spiral_r.columns[0]], spiral_r[spiral_r.columns[1]]\n",
    "spiral_xk, spiral_yk = spiral_k[spiral_k.columns[0]], spiral_k[spiral_k.columns[1]]\n",
    "    \n",
    "markersize_ = 7**2\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.scatter(spiral_xr, spiral_yr, c = 'r', s=markersize_)\n",
    "plt.scatter(spiral_xk, spiral_yk, c = 'k', s=markersize_)\n",
    "plt.axis([-1.5, 1.5, -1.5, 1.5])\n",
    "plt.title(\"Spiral\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "H2O in Python is designed to be very similar in look and feel to to scikit-learn. Models are initialized individually with desired or default parameters and then trained on data.\n",
    "\n",
    "Note that the below examples use model.train(), as opposed the traditional model.fit() This is because h2o-py takes the data frame AND column indices for the feature and response columns, while scikit-learn takes in feature frames.\n",
    "\n",
    "H2O supports model.fit() so that it can be incorporated into a scikit-learn pipeline, but we advise using train() in all other cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spiral.col_names[0:2]\n",
    "y = spiral.col_names[2]\n",
    "dl_model = H2ODeepLearningEstimator(epochs=1000)\n",
    "dl_model.train(X, y, spiral)\n",
    "\n",
    "gbm_model = H2OGradientBoostingEstimator()\n",
    "gbm_model.train(X, y, spiral)\n",
    "\n",
    "drf_model = H2ORandomForestEstimator()\n",
    "drf_model.train(X, y, spiral)\n",
    "\n",
    "glm_model = H2OGeneralizedLinearEstimator(family=\"binomial\")\n",
    "glm_model.fit(spiral[X], spiral[y])                                #model.fit() example\n",
    "\n",
    "models = [dl_model, gbm_model, drf_model, glm_model]\n",
    "m_names = [\"Deep Learning\", \"Gradient Boosted Method\", \"Distributed Random Forest\", \"Generalized Linear Model\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained four models to classify points as red or black based on their (x,y) coordinates.\n",
    "To see how our models have performed, we ask them to predict the colors of the grid.\n",
    "\n",
    "Since we'll be doing a lot of spiral plotting, let's write a little helper function to keep things clean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spirals(models, model_names):\n",
    "    fig, ax = plt.subplots(2,2, figsize=(12,12))\n",
    "    for k, subplot in enumerate(ax.flatten()):\n",
    "        subplot.scatter(spiral_xr, spiral_yr, c = 'r', s=markersize_)\n",
    "        subplot.scatter(spiral_xk, spiral_yk, c = 'k', s=markersize_)\n",
    "        subplot.axis([-1.5, 1.5, -1.5, 1.5])\n",
    "        subplot.set_title(model_names[k])\n",
    "        subplot.set_xlabel('x')\n",
    "        subplot.set_ylabel('y')\n",
    "        pred_z = models[k].predict(grid).as_data_frame(True)\n",
    "        subplot.contour(grid_x, grid_y, (pred_z['predict'] == 'Black').astype(np.int).values.reshape(201,201), colors='b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are four graphs of the contour plots of the predictions, so that we can see how exactly the algorithms grouped the points into black and red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spirals(models, m_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Deeper Dive into Deep Learning\n",
    "Now let's explore the evolution of our deep learning model over training time (number of passes over the data, aka epochs).\n",
    "We will use checkpointing to ensure that we continue training the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_1 = H2ODeepLearningEstimator(epochs=1)\n",
    "dl_1.train(X, y, spiral)\n",
    "\n",
    "dl_250 = H2ODeepLearningEstimator(checkpoint=dl_1, epochs=250)\n",
    "dl_250.train(X, y, spiral)\n",
    "\n",
    "dl_500 = H2ODeepLearningEstimator(checkpoint=dl_250, epochs=500)\n",
    "dl_500.train(X, y, spiral)\n",
    "\n",
    "dl_750 = H2ODeepLearningEstimator(checkpoint=dl_500, epochs=750)\n",
    "dl_750.train(X, y, spiral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the network learns the structure of the spirals with enough training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dl = [dl_1, dl_250, dl_500, dl_750]\n",
    "m_names_dl = [\"DL \" + str(int(model.get_params()['epochs']['actual_value'])) + \\\n",
    "                                     \" Epochs\" for model in models_dl]\n",
    "\n",
    "plot_spirals(models_dl, m_names_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Network Architecture\n",
    "Of course, there is far more to constructing Deep Learning models than simply having them run longer.\n",
    "Consider the four following setups.\n",
    "\n",
    "* Single layer, 1000 nodes\n",
    "* Two layers, 200 nodes each\n",
    "* Three layers, 42 nodes each\n",
    "* Four layers, 11 -> 13 -> 17 -> 19\n",
    "\n",
    "The H2O Architecture uses the hidden keyword to control model network architecture.\n",
    "Hidden takes a list of integers, representing the number of nodes in each layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_1 = H2ODeepLearningEstimator(hidden=[1000], epochs=500)\n",
    "dl_1.train(X, y, spiral)\n",
    "\n",
    "dl_2 = H2ODeepLearningEstimator(hidden=[200,200], epochs=500)\n",
    "dl_2.train(X, y, spiral)\n",
    "\n",
    "dl_3 = H2ODeepLearningEstimator(hidden=[42,42,42], epochs=500)\n",
    "dl_3.train(X, y, spiral)\n",
    "\n",
    "dl_4 = H2ODeepLearningEstimator(hidden=[11,13,17,19], epochs = 1000)\n",
    "dl_4.train(X, y, spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that different configurations can achieve similar performance, and that tuning will be required for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_network = [dl_1, dl_2, dl_3, dl_4]\n",
    "m_names_network = [\"1000\", \"200 x 200\", \"42 x 42 x 42\", \"11 x 13 x 17 x 19\"]\n",
    "\n",
    "plot_spirals(models_network, m_names_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "Next, we compare between different activation functions, including one with 50% dropout regularization in the hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_act = []\n",
    "m_names_act = []\n",
    "for i,method in enumerate([\"Tanh\",\"Maxout\",\"Rectifier\",\"RectifierWithDropout\"]):\n",
    "    models_act.append(H2ODeepLearningEstimator(activation=method, hidden=[100,100], epochs=1000))\n",
    "    models_act[i].train(X, y, spiral)\n",
    "    m_names_act.append(\"DL \"+ method + \" Activation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spirals(models_act, m_names_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the dropout rate was too high or the number of epochs was too low for the last configuration, which often ends up performing the best on larger datasets where generalization is important.\n",
    "\n",
    "More information about the parameters can be found in the H2O Deep Learning booklet.\n",
    "# Covertype Dataset\n",
    "The following examples use the Covertype dataset from UC Irvine, which concerns predicting forest cover based on cartographical data.\n",
    "We import the full covertype dataset (581k rows, 13 columns, 10 numerical, 3 categorical) and then split the data 3 ways:\n",
    "\n",
    "* 60% for training\n",
    "* 20% for validation (hyper parameter tuning)\n",
    "* 20% for final testing\n",
    "\n",
    "We will train a data set on one set and use the others to test the validity of the model by ensuring that it can predict accurately on data the model has not been shown.\n",
    "\n",
    "The second set will be used for validation most of the time.\n",
    "\n",
    "The third set will be withheld until the end, to ensure that our validation accuracy is consistent with data we have never seen during the iterative process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_df = h2o.import_file(path = os.path.realpath(\"data/covtype.full.csv\"))\n",
    "\n",
    "#split the data as described above\n",
    "train, valid, test = covtype_df.split_frame([0.6, 0.2], seed=1234)\n",
    "\n",
    "#Prepare predictors and response columns\n",
    "covtype_X = covtype_df.col_names[:-1]     #last column is cover_type, \n",
    "covtype_y = covtype_df.col_names[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Impressions\n",
    "Let's run our first Deep Learning model on the covtype dataset.\n",
    "We want to predict the Cover_Type column, a categorical feature with 7 levels, and the Deep Learning model will be tasked to perform (multi-class) classification. It uses the other 12 predictors of the dataset, of which 10 are numerical, and 2 are categorical with a total of 44 levels.\n",
    "\n",
    "We can expect the Deep Learning model to have 56 input neurons (after automatic one-hot encoding). First run will be only one epoch to get a feel for the model construction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the model_id for easy lookup in Flow\n",
    "covtype_model_v1 = H2ODeepLearningEstimator(model_id=\"covtype_v1\", epochs=1, variable_importances=True)\n",
    "covtype_model_v1.train(covtype_X, covtype_y, training_frame = train, validation_frame = valid)\n",
    "print(covtype_model_v1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the model in Flow for more information about model building etc.\n",
    "\n",
    "Enter getModel \"covtype_v1\" into a cell and run in place pressing Ctrl-Enter. Alternatively, you can click on the Models tab, select List All Models, and click on the model named \"covtype_v1\" as specified in our model construction above.\n",
    "\n",
    "## Variable Importances\n",
    "Variable importances for Neural Network models are notoriously difficult to compute, and there are many pitfalls. H2O Deep Learning has implemented the method of Gedeon, and returns relative variable importances in descending order of importance.\n",
    "\n",
    "Note that we have exactly 56 input neurons, as expected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = pd.DataFrame(covtype_model_v1.varimp(), \n",
    "             columns=[\"Variable\", \"Relative Importance\", \"Scaled Importance\", \"Percentage\"])\n",
    "print(var_df.shape)\n",
    "var_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "Now we run another, smaller network and we let it stop automatically once the misclassification rate converges (specifically if the moving average of length 2 does not improve by at least 1% for 2 consecutive scoring events).\n",
    "\n",
    "We also sample the validation set to 10,000 rows for faster scoring.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_model_v2 = H2ODeepLearningEstimator(\n",
    "    model_id=\"covtype_v2\", \n",
    "    hidden=[32,32,32],                  ## small network, runs faster\n",
    "    epochs=1000000,                      ## hopefully converges earlier...\n",
    "    score_validation_samples=10000,      ## sample the validation dataset (faster)\n",
    "    stopping_rounds=2,\n",
    "    stopping_metric=\"misclassification\", ## alternatives: \"MSE\",\"logloss\",\"r2\"\n",
    "    stopping_tolerance=0.01)\n",
    "covtype_model_v2.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the scoring history, we can look up our new model in Flow.\n",
    "\n",
    "Alternatively, we can use the score_history method to retrieve the data as a pandas DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_v2_df = covtype_model_v2.score_history()\n",
    "cov_v2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cov_v2_df['training_classification_error'], label=\"training_classification_error\")\n",
    "plt.plot(cov_v2_df['validation_classification_error'], label=\"validation_classification_error\")\n",
    "plt.title(\"Covertype Deep Learner (Early Stop)\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Learning Rate\n",
    "By default, H2O Deep Learning uses an adaptive learning rate (ADADELTA) for its stochastic gradient descent optimization. There are only two tuning parameters for this method: rho and epsilon, which balance the global and local search efficiencies. rho is the similarity to prior weight updates (similar to momentum), and epsilon is a parameter that prevents the optimization to get stuck in local optima.\n",
    "\n",
    "Defaults are rho=0.99 and epsilon=1e-8. For cases where convergence speed is very important, it might make sense to perform a few runs to optimize these two parameters (e.g., with rho in c(0.9,0.95,0.99,0.999) and epsilon in c(1e-10,1e-8,1e-6,1e-4)). Of course, as always with grid searches, caution has to be applied when extrapolating grid search results to a different parameter regime (e.g., for more epochs or different layer topologies or activation functions, etc.).\n",
    "\n",
    "If adaptive_rate is disabled, several manual learning rate parameters become important: rate, rate_annealing, rate_decay,  momentum_start, momentum_ramp, momentum_stable and nesterov_accelerated_gradient, the discussion of which we leave to H2O Deep Learning booklet.\n",
    "\n",
    "## Tuning\n",
    "With some tuning, it is possible to obtain less than 10% test set error rate in about one minute. Error rates of below 5% are possible with larger models. Deep tree methods are more effective for this dataset than Deep Learning, as the space needs to be simply be partitioned into the corresponding hyper-space corners to solve this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_model_tuned = H2ODeepLearningEstimator(\n",
    "  model_id=\"covtype_tuned\",\n",
    "  overwrite_with_best_model=False,\n",
    "  hidden=[128,128,128],            # more hidden layers -> more complex interactions\n",
    "  epochs=10,                       # to keep it short enough\n",
    "  score_validation_samples=10000,  # downsample validation set for faster scoring\n",
    "  score_duty_cycle=0.025,          # don't score more than 2.5% of the wall time\n",
    "  adaptive_rate=False,             # manually tuned learning rate\n",
    "  rate=0.01, \n",
    "  rate_annealing=0.000002,            \n",
    "  momentum_start=0.2,              # manually tuned momentum\n",
    "  momentum_stable=0.4, \n",
    "  momentum_ramp=10000000, \n",
    "  l1=0.00001,                      # add some L1/L2 regularization\n",
    "  l2=0.00001,\n",
    "  max_w2=10.                       # helps stability for Rectifier\n",
    ")\n",
    "covtype_model_tuned.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the training error with the validation and test set errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_tuned_df = covtype_model_tuned.scoring_history()\n",
    "cov_tuned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cov_tuned_df['training_classification_error'], label=\"training_classification_error\")\n",
    "plt.plot(cov_tuned_df['validation_classification_error'], label=\"validation_classification_error\")\n",
    "plt.title(\"Covertype Deep Learner (Tuned)\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = covtype_model_tuned.predict(test[0:-1]).as_data_frame(use_pandas=True)\n",
    "test_actual = test.as_data_frame(use_pandas=True)['Cover_Type']\n",
    "(test_actual == pred['predict']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've achieved approximately 90% accuracy on our test data, which is fairly in line with our validation results.\n",
    "\n",
    "## Shutdown H2O Cluster\n",
    "This concludes the Python portion of the deeplearning demo.\n",
    "\n",
    "Shut down the cluster now that we are done using it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
